{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uENyDuyNLl0_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Definir el conjunto de datos\n",
        "texts = [\n",
        "    \"la casa es grande\",\n",
        "    \"el perro corre rápido\",\n",
        "    \"me gusta el verano\",\n",
        "    \"ella canta muy bien\",\n",
        "    \"la comida es deliciosa\",\n",
        "    \"él juega en el parque\",\n",
        "    \"ella tiene un gato\",\n",
        "    \"la luna brilla fuerte\",\n",
        "    \"el coche es rápido\",\n",
        "    \"el gato duerme mucho\",\n",
        "    \"el sol brilla en verano\",\n",
        "    \"me gusta comer pizza\"\n",
        "    # Puedes agregar más frases para mejorar el entrenamiento\n",
        "]\n",
        "\n",
        "# 2. Preprocesamiento de datos\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Crear secuencias de entrada y salida\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "\n",
        "for line in texts:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  #print(token_list)\n",
        "\n",
        "  for i in range(1, len(token_list)):\n",
        "    input_sequences.append(token_list[:i])\n",
        "    output_words.append(token_list[i])\n",
        "\n",
        "# Padding de secuencias\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\")\n",
        "output_words = to_categorical(output_words, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1YjYpoKoH4r",
        "outputId": "4b9ea11d-7aa8-4993-b5d9-15e0be1486cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  2],\n",
              "       [ 0,  0,  2, 12],\n",
              "       [ 0,  2, 12,  3],\n",
              "       [ 0,  0,  0,  1],\n",
              "       [ 0,  0,  1, 14],\n",
              "       [ 0,  1, 14, 15],\n",
              "       [ 0,  0,  0,  5],\n",
              "       [ 0,  0,  5,  6],\n",
              "       [ 0,  5,  6,  1],\n",
              "       [ 0,  0,  0,  8],\n",
              "       [ 0,  0,  8, 16],\n",
              "       [ 0,  8, 16, 17],\n",
              "       [ 0,  0,  0,  2],\n",
              "       [ 0,  0,  2, 19],\n",
              "       [ 0,  2, 19,  3],\n",
              "       [ 0,  0,  0, 21],\n",
              "       [ 0,  0, 21, 22],\n",
              "       [ 0, 21, 22,  9],\n",
              "       [21, 22,  9,  1],\n",
              "       [ 0,  0,  0,  8],\n",
              "       [ 0,  0,  8, 24],\n",
              "       [ 0,  8, 24, 25],\n",
              "       [ 0,  0,  0,  2],\n",
              "       [ 0,  0,  2, 26],\n",
              "       [ 0,  2, 26, 11],\n",
              "       [ 0,  0,  0,  1],\n",
              "       [ 0,  0,  1, 28],\n",
              "       [ 0,  1, 28,  3],\n",
              "       [ 0,  0,  0,  1],\n",
              "       [ 0,  0,  1, 10],\n",
              "       [ 0,  1, 10, 29],\n",
              "       [ 0,  0,  0,  1],\n",
              "       [ 0,  0,  1, 31],\n",
              "       [ 0,  1, 31, 11],\n",
              "       [ 1, 31, 11,  9],\n",
              "       [ 0,  0,  0,  5],\n",
              "       [ 0,  0,  5,  6],\n",
              "       [ 0,  5,  6, 32]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GJTfTpyoIW5",
        "outputId": "dacde8a8-d3f8-48ad-cd9f-ce571f89ccb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definir el modelo Seq2Seq\n",
        "latent_dim = 50  # Dimensión del espacio latente\n",
        "\n",
        "# Codificador\n",
        "encoder_inputs = Input(shape=(max_sequence_len,))\n",
        "encoder_embedding = Embedding(input_dim=total_words, output_dim=latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decodificador\n",
        "decoder_inputs = Input(shape=(1,))  # Usaremos una sola palabra a la vez\n",
        "decoder_embedding = Embedding(input_dim=total_words, output_dim=latent_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(total_words, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Aplanar la salida del decodificador para que coincida con la forma del objetivo\n",
        "decoder_outputs = Flatten()(decoder_outputs)\n",
        "\n",
        "# Modelo completo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Mostrar la arquitectura del modelo\n",
        "model.summary()\n",
        "\n",
        "# 4. Entrenar el modelo\n",
        "# Usamos la última palabra como entrada inicial para el decodificador en cada paso\n",
        "decoder_input_data = np.zeros((len(input_sequences), 1))\n",
        "model.fit([input_sequences, decoder_input_data], output_words, batch_size=16, epochs=300)  # Aumentar el número de épocas"
      ],
      "metadata": {
        "id": "ncuq827Jn1-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Función de inferencia para generar la próxima palabra\n",
        "def generate_next_word(seed_text, num_words):\n",
        "  for _ in range(num_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding=\"pre\")\n",
        "    predicted = model.predict([token_list, np.zeros((1, 1))], verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
        "\n",
        "    # Validar si el índice de la palabra está en el vocabulario\n",
        "    if predicted_word_index[0] in tokenizer.index_word:\n",
        "      predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "      seed_text += \" \" + predicted_word\n",
        "    else:\n",
        "      break\n",
        "  return seed_text"
      ],
      "metadata": {
        "id": "kCOCUlS9kpIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Ejemplo de uso\n",
        "seed_text = \"la comida\"\n",
        "generated_text = generate_next_word(seed_text, 2)\n",
        "print(\"Texto generado:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKBr_fq6Mxm6",
        "outputId": "f2752ab4-dabc-4f64-e635-62ed06d1b9bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto generado: la comida es deliciosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Ejemplo de uso\n",
        "seed_text = \"el loro\"\n",
        "generated_text = generate_next_word(seed_text, 2)\n",
        "print(\"Texto generado:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwAIahaOMyKS",
        "outputId": "6bc65ba7-1743-4bc0-a6f5-e24521895ac9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto generado: el loro gato duerme\n"
          ]
        }
      ]
    }
  ]
}